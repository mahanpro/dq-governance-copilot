{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe744b38-96d6-4873-ae1c-c909db844d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG dq_demo;\n",
    "USE SCHEMA core;\n",
    "\n",
    "TRUNCATE TABLE dq_ground_truth;\n",
    "TRUNCATE TABLE dq_incidents;\n",
    "TRUNCATE TABLE dq_profiles;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efe8782-55f4-4815-8722-d191d087da35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS dq_demo;\n",
    "USE CATALOG dq_demo;\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS core;\n",
    "USE SCHEMA core;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7b0d5e6-8d7d-4050-a841-92dd3722c641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "spark.sql(\"USE CATALOG dq_demo\")\n",
    "spark.sql(\"USE SCHEMA core\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e1f3b39-52b4-45e9-989d-7b0fd788c62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import uuid\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# 1. Date range\n",
    "start_date = date(2024, 1, 1)\n",
    "num_days = 90\n",
    "dates = [start_date + timedelta(days=i) for i in range(num_days)]\n",
    "\n",
    "date_df = spark.createDataFrame(\n",
    "    [(d,) for d in dates],\n",
    "    schema=T.StructType([T.StructField(\"batch_date\", T.DateType(), False)])\n",
    ")\n",
    "\n",
    "# 2. Customers (simple synthetic)\n",
    "num_customers = 5000\n",
    "customers_df = (\n",
    "    spark.range(0, num_customers)\n",
    "    .withColumn(\"customer_id\", F.concat(F.lit(\"C\"), F.col(\"id\")))\n",
    "    .withColumn(\n",
    "        \"signup_date\",\n",
    "        F.date_sub(\n",
    "            F.lit(start_date),\n",
    "            F.floor(F.rand() * 365).cast(\"int\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"segment\", F.when(F.rand() < 0.3, F.lit(\"premium\")).otherwise(F.lit(\"standard\")))\n",
    "    .withColumn(\"region\", F.when(F.rand() < 0.5, F.lit(\"NA\")).otherwise(F.lit(\"EU\")))\n",
    "    .drop(\"id\")\n",
    ")\n",
    "\n",
    "# Write customers as Delta\n",
    "(\n",
    "    customers_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(\"dq_demo.core.customers\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d77dc6-97ef-4ce0-9e46-7a698a06ff23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Transactions\n",
    "import random\n",
    "\n",
    "rows = []\n",
    "for d in dates:\n",
    "    # variable number of transactions per day\n",
    "    n_tx = random.randint(2000, 4000)\n",
    "    for i in range(n_tx):\n",
    "        cust_id = f\"C{random.randint(0, num_customers-1)}\"\n",
    "        amount = max(1.0, random.gauss(100.0, 30.0))\n",
    "        payment_method = random.choices(\n",
    "            [\"card\", \"cash\", \"finance\"],\n",
    "            weights=[0.6, 0.25, 0.15],\n",
    "        )[0]\n",
    "        region = random.choice([\"NA\", \"EU\"])\n",
    "        rows.append((str(uuid.uuid4()), cust_id, d, amount, payment_method, region, d))\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"transaction_id\", T.StringType(), False),\n",
    "    T.StructField(\"customer_id\", T.StringType(), False),\n",
    "    T.StructField(\"event_time\", T.DateType(), False),\n",
    "    T.StructField(\"amount\", T.DoubleType(), False),\n",
    "    T.StructField(\"payment_method\", T.StringType(), False),\n",
    "    T.StructField(\"region\", T.StringType(), False),\n",
    "    T.StructField(\"created_date\", T.DateType(), False),\n",
    "])\n",
    "\n",
    "transactions_df = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "(\n",
    "    transactions_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .partitionBy(\"created_date\")\n",
    "    .saveAsTable(\"dq_demo.core.transactions\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b641fc2-f642-48d3-bb96-d31b7ad19353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Reload transactions\n",
    "tx = spark.table(\"dq_demo.core.transactions\")\n",
    "\n",
    "# Example: for a specific day, make many customer_id null => NULL_SPIKE\n",
    "target_date = start_date + timedelta(days=30)\n",
    "\n",
    "to_corrupt = (\n",
    "    tx\n",
    "    .where(F.col(\"created_date\") == F.lit(target_date))\n",
    "    .withColumn(\"row_num\", F.row_number().over(Window.orderBy(\"transaction_id\")))\n",
    ")\n",
    "\n",
    "# corrupt 30% of rows\n",
    "corrupted = (\n",
    "    to_corrupt\n",
    "    .withColumn(\n",
    "        \"customer_id\",\n",
    "        F.when(F.col(\"row_num\") <= F.col(\"row_num\") * 0 + int(to_corrupt.count() * 0.3),\n",
    "               F.lit(None).cast(\"string\"))\n",
    "        .otherwise(F.col(\"customer_id\"))\n",
    "    )\n",
    "    .drop(\"row_num\")\n",
    ")\n",
    "\n",
    "# Overwrite that partition\n",
    "clean_others = tx.where(F.col(\"created_date\") != F.lit(target_date))\n",
    "\n",
    "tx_new = clean_others.unionByName(corrupted)\n",
    "\n",
    "(\n",
    "    tx_new\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .partitionBy(\"created_date\")\n",
    "    .saveAsTable(\"dq_demo.core.transactions\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a084610-a2b3-4ad6-ba9d-a24085551d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "gt_rows = [\n",
    "    Row(\n",
    "        id=str(uuid.uuid4()),\n",
    "        table_name=\"transactions\",\n",
    "        column_name=\"customer_id\",\n",
    "        batch_date=target_date,\n",
    "        anomaly_type=\"NULL_SPIKE\",\n",
    "        injected_value_stats=\"30% customer_id set to NULL on this date\"\n",
    "    )\n",
    "]\n",
    "\n",
    "gt_schema = T.StructType([\n",
    "    T.StructField(\"id\", T.StringType(), False),\n",
    "    T.StructField(\"table_name\", T.StringType(), False),\n",
    "    T.StructField(\"column_name\", T.StringType(), False),\n",
    "    T.StructField(\"batch_date\", T.DateType(), False),\n",
    "    T.StructField(\"anomaly_type\", T.StringType(), False),\n",
    "    T.StructField(\"injected_value_stats\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "gt_df = spark.createDataFrame(gt_rows, schema=gt_schema)\n",
    "\n",
    "(\n",
    "    gt_df\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(\"dq_demo.core.dq_ground_truth\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee025af0-a51c-4ddc-9d72-6f03f971198b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"dq_demo.core.customers\").limit(5).show()\n",
    "spark.table(\"dq_demo.core.transactions\").groupBy(\"created_date\").count().orderBy(\"created_date\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9adf351d-80e3-4b8c-9401-616c4f133424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 6. AMOUNT_DRIFT anomaly on a later date\n",
    "drift_date = start_date + timedelta(days=45)  # 2024-02-15\n",
    "\n",
    "tx = spark.table(\"dq_demo.core.transactions\")\n",
    "\n",
    "drift_partition = tx.where(F.col(\"created_date\") == F.lit(drift_date))\n",
    "other_partitions = tx.where(F.col(\"created_date\") != F.lit(drift_date))\n",
    "\n",
    "# Inflate amount and add noise to simulate drift\n",
    "drifted = (\n",
    "    drift_partition\n",
    "    .withColumn(\"amount\", F.col(\"amount\") * 3.0 + F.rand() * 10.0)\n",
    ")\n",
    "\n",
    "tx_with_drift = other_partitions.unionByName(drifted)\n",
    "\n",
    "(\n",
    "    tx_with_drift\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .partitionBy(\"created_date\")\n",
    "    .saveAsTable(\"dq_demo.core.transactions\")\n",
    ")\n",
    "\n",
    "# Ground truth row for AMOUNT_DRIFT\n",
    "gt_rows_drift = [\n",
    "    Row(\n",
    "        id=str(uuid.uuid4()),\n",
    "        table_name=\"transactions\",\n",
    "        column_name=\"amount\",\n",
    "        batch_date=drift_date,\n",
    "        anomaly_type=\"AMOUNT_DRIFT\",\n",
    "        injected_value_stats=\"amount multiplied by 3 and noise added on this date\"\n",
    "    )\n",
    "]\n",
    "\n",
    "gt_df_drift = spark.createDataFrame(gt_rows_drift, schema=gt_schema)\n",
    "\n",
    "(\n",
    "    gt_df_drift\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(\"dq_demo.core.dq_ground_truth\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656d34e6-c5dd-4f38-be9f-dd429343812e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. VOLUME_DROP anomaly (fewer rows than normal)\n",
    "volume_drop_date = start_date + timedelta(days=60)  # 2024-03-01\n",
    "\n",
    "tx = spark.table(\"dq_demo.core.transactions\")\n",
    "\n",
    "drop_partition = tx.where(F.col(\"created_date\") == F.lit(volume_drop_date))\n",
    "other_partitions = tx.where(F.col(\"created_date\") != F.lit(volume_drop_date))\n",
    "\n",
    "# Keep only 20 percent of rows on that date\n",
    "fraction = 0.2\n",
    "dropped = drop_partition.sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "\n",
    "tx_with_drop = other_partitions.unionByName(dropped)\n",
    "\n",
    "(\n",
    "    tx_with_drop\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .partitionBy(\"created_date\")\n",
    "    .saveAsTable(\"dq_demo.core.transactions\")\n",
    ")\n",
    "\n",
    "# Ground truth row for VOLUME_DROP on the table (row_count anomaly)\n",
    "gt_rows_drop = [\n",
    "    Row(\n",
    "        id=str(uuid.uuid4()),\n",
    "        table_name=\"transactions\",\n",
    "        column_name=\"*ROW_COUNT*\",  # indicate it is about volume, not one column\n",
    "        batch_date=volume_drop_date,\n",
    "        anomaly_type=\"VOLUME_DROP\",\n",
    "        injected_value_stats=f\"row_count reduced to {int(fraction*100)}% on this date\"\n",
    "    )\n",
    "]\n",
    "\n",
    "gt_df_drop = spark.createDataFrame(gt_rows_drop, schema=gt_schema)\n",
    "\n",
    "(\n",
    "    gt_df_drop\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(\"dq_demo.core.dq_ground_truth\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558e0865-8e3d-46b8-a7bf-bed53d21933e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.table(\"dq_demo.core.dq_ground_truth\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5865355488869714,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_generate_synthetic_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
