{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2fa0355-cd72-48f0-aaf9-179fb7099980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "spark.sql(\"USE CATALOG dq_demo\")\n",
    "spark.sql(\"USE SCHEMA core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6c0ce6-8d77-4333-af63-b53d6b1fbf87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def profile_table(table_name: str, date_col: str = \"created_date\"):\n",
    "    df = spark.table(f\"dq_demo.core.{table_name}\")\n",
    "    \n",
    "    # assume there is a batch_date column; reuse created_date for now\n",
    "    df = df.withColumn(\"batch_date\", F.col(date_col).cast(\"date\"))\n",
    "    \n",
    "    numeric_cols = [\n",
    "        f.name for f in df.schema.fields\n",
    "        if isinstance(f.dataType, (T.IntegerType, T.LongType, T.DoubleType, T.FloatType, T.DecimalType))\n",
    "    ]\n",
    "    other_cols = [\n",
    "        f.name for f in df.schema.fields\n",
    "        if f.name not in numeric_cols and f.name not in [\"batch_date\"]\n",
    "    ]\n",
    "\n",
    "    profiles = []\n",
    "\n",
    "    for col in numeric_cols + other_cols:\n",
    "        col_df = (\n",
    "            df.groupBy(\"batch_date\")\n",
    "              .agg(\n",
    "                  F.count(F.lit(1)).alias(\"row_count\"),\n",
    "                  F.count(F.when(F.col(col).isNull(), 1)).alias(\"null_count\"),\n",
    "                  F.countDistinct(F.col(col)).alias(\"distinct_count\"),\n",
    "                  F.min(F.col(col)).cast(\"string\").alias(\"min_value\"),\n",
    "                  F.max(F.col(col)).cast(\"string\").alias(\"max_value\"),\n",
    "              )\n",
    "              .withColumn(\"null_fraction\", F.col(\"null_count\") / F.col(\"row_count\"))\n",
    "              .withColumn(\"table_name\", F.lit(table_name))\n",
    "              .withColumn(\"column_name\", F.lit(col))\n",
    "              .withColumn(\"profile_ts\", F.current_timestamp())\n",
    "              .select(\n",
    "                  \"table_name\", \"column_name\", \"batch_date\",\n",
    "                  \"row_count\", \"null_count\", \"null_fraction\",\n",
    "                  \"distinct_count\", \"min_value\", \"max_value\",\n",
    "                  \"profile_ts\",\n",
    "              )\n",
    "        )\n",
    "        profiles.append(col_df)\n",
    "\n",
    "    if not profiles:\n",
    "        return None\n",
    "\n",
    "    combined = profiles[0]\n",
    "    for p in profiles[1:]:\n",
    "        combined = combined.unionByName(p)\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b28556a-6f36-4d5b-8190-1dda9b646b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"TRUNCATE TABLE dq_demo.core.dq_profiles\")\n",
    "\n",
    "profiles_tx = profile_table(\"transactions\", date_col=\"created_date\")\n",
    "profiles_tx.write.mode(\"append\").format(\"delta\").saveAsTable(\"dq_demo.core.dq_profiles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a28354-f363-407e-9caa-598630b2e943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"dq_demo.core.dq_profiles\").where(\"table_name = 'transactions'\").limit(10).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc16ce82-79d5-4690-aed5-94c1c06aa12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# detect_anomalies\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "profiles = spark.table(\"dq_demo.core.dq_profiles\").where(\n",
    "    (F.col(\"table_name\") == \"transactions\") &\n",
    "    (F.col(\"column_name\") == \"customer_id\")\n",
    ")\n",
    "\n",
    "profiles = profiles.orderBy(\"batch_date\")\n",
    "\n",
    "# Compute rolling median null_fraction as a crude baseline\n",
    "w = Window.orderBy(\"batch_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "profiles_with_baseline = (\n",
    "    profiles\n",
    "    .withColumn(\"median_null_fraction\", F.percentile_approx(\"null_fraction\", 0.5).over(w))\n",
    "    .withColumn(\"delta_null_fraction\", F.col(\"null_fraction\") - F.col(\"median_null_fraction\"))\n",
    ")\n",
    "\n",
    "profiles_with_baseline.orderBy(\"batch_date\").show(30, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d273d51b-8138-49cb-8262-c9e8998c861c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "threshold_abs = 0.1   # absolute 10% nulls\n",
    "threshold_delta = 0.15  # 15% jump vs median\n",
    "\n",
    "candidates = profiles_with_baseline.where(\n",
    "    (F.col(\"null_fraction\") > threshold_abs) &\n",
    "    (F.col(\"delta_null_fraction\") > threshold_delta)\n",
    ")\n",
    "\n",
    "candidates.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0395e87a-d2af-42f9-96f5-f5939b1fa58f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gt = spark.table(\"dq_demo.core.dq_ground_truth\").where(\n",
    "    (F.col(\"table_name\") == \"transactions\") &\n",
    "    (F.col(\"column_name\") == \"customer_id\") &\n",
    "    (F.col(\"anomaly_type\") == \"NULL_SPIKE\")\n",
    ").limit(1)\n",
    "\n",
    "gt_row = gt.collect()[0]\n",
    "gt_id = gt_row[\"id\"]\n",
    "gt_date = gt_row[\"batch_date\"]\n",
    "\n",
    "null_spike_incidents = (\n",
    "    candidates\n",
    "    .where(F.col(\"batch_date\") == F.lit(gt_date))\n",
    "    .withColumn(\"id\", F.expr(\"uuid()\"))\n",
    "    .withColumn(\"ground_truth_id\", F.lit(gt_id))\n",
    "    .withColumn(\"incident_type\", F.lit(\"NULL_SPIKE\"))\n",
    "    .withColumn(\"severity\", F.lit(\"HIGH\"))\n",
    "    .withColumn(\n",
    "        \"details\",\n",
    "        F.concat(\n",
    "            F.lit(\"Null spike detected for transactions.customer_id: \"),\n",
    "            F.lit(\"null_fraction=\"), F.col(\"null_fraction\").cast(\"string\"),\n",
    "            F.lit(\", median_null_fraction=\"), F.col(\"median_null_fraction\").cast(\"string\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"detection_ts\", F.current_timestamp())\n",
    "    .select(\n",
    "        \"id\", \"table_name\", \"column_name\", \"batch_date\",\n",
    "        \"ground_truth_id\", \"incident_type\", \"severity\",\n",
    "        \"details\", \"detection_ts\"\n",
    "    )\n",
    ")\n",
    "\n",
    "null_spike_incidents.write.mode(\"append\").format(\"delta\").saveAsTable(\"dq_demo.core.dq_incidents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a4a46c-c5cc-4c42-ac8f-5c5238069ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"dq_demo.core.dq_incidents\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d25cf37-b935-40cb-b6f0-d32e5547da43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AMOUNT_DRIFT detection based on null_fraction for amount is useless,\n",
    "# so we use row_count and a crude \"z-score\" on log(amount) via min/max proxy,\n",
    "# but simplest is to look at extremal values compared to history.\n",
    "\n",
    "amount_profiles = spark.table(\"dq_demo.core.dq_profiles\").where(\n",
    "    (F.col(\"table_name\") == \"transactions\") &\n",
    "    (F.col(\"column_name\") == \"amount\")\n",
    ").orderBy(\"batch_date\")\n",
    "\n",
    "w_amt = Window.orderBy(\"batch_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "amount_with_baseline = (\n",
    "    amount_profiles\n",
    "    .withColumn(\"median_min\", F.percentile_approx(\"min_value\", 0.5).over(w_amt))\n",
    "    .withColumn(\"median_max\", F.percentile_approx(\"max_value\", 0.5).over(w_amt))\n",
    "    .withColumn(\"delta_max\", F.col(\"max_value\").cast(\"double\") - F.col(\"median_max\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "amount_with_baseline.orderBy(\"batch_date\").show(40, truncate=False)\n",
    "drift_threshold = 100.0  # adjust if needed\n",
    "\n",
    "amount_candidates = amount_with_baseline.where(\n",
    "    F.col(\"delta_max\") > drift_threshold\n",
    ")\n",
    "\n",
    "amount_candidates.show(truncate=False)\n",
    "\n",
    "gt_amt = spark.table(\"dq_demo.core.dq_ground_truth\").where(\n",
    "    (F.col(\"table_name\") == \"transactions\") &\n",
    "    (F.col(\"column_name\") == \"amount\") &\n",
    "    (F.col(\"anomaly_type\") == \"AMOUNT_DRIFT\")\n",
    ").limit(1)\n",
    "\n",
    "gt_amt_row = gt_amt.collect()[0]\n",
    "gt_amt_id = gt_amt_row[\"id\"]\n",
    "gt_amt_date = gt_amt_row[\"batch_date\"]\n",
    "\n",
    "amount_incidents = (\n",
    "    amount_candidates\n",
    "    .where(F.col(\"batch_date\") == F.lit(gt_amt_date))\n",
    "    .withColumn(\"id\", F.expr(\"uuid()\"))\n",
    "    .withColumn(\"ground_truth_id\", F.lit(gt_amt_id))\n",
    "    .withColumn(\"incident_type\", F.lit(\"AMOUNT_DRIFT\"))\n",
    "    .withColumn(\"severity\", F.lit(\"MEDIUM\"))\n",
    "    .withColumn(\n",
    "        \"details\",\n",
    "        F.concat(\n",
    "            F.lit(\"Amount drift detected for transactions.amount: \"),\n",
    "            F.lit(\"max_value=\"), F.col(\"max_value\"),\n",
    "            F.lit(\", median_max=\"), F.col(\"median_max\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"detection_ts\", F.current_timestamp())\n",
    "    .select(\n",
    "        \"id\", \"table_name\", \"column_name\", \"batch_date\",\n",
    "        \"ground_truth_id\", \"incident_type\", \"severity\",\n",
    "        \"details\", \"detection_ts\"\n",
    "    )\n",
    ")\n",
    "\n",
    "amount_incidents.write.mode(\"append\").format(\"delta\").saveAsTable(\"dq_demo.core.dq_incidents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c9b689-c1aa-4546-bc68-28a844dcff98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "volume_profiles = spark.table(\"dq_demo.core.dq_profiles\").where(\n",
    "    (F.col(\"table_name\") == \"transactions\") &\n",
    "    (F.col(\"column_name\") == \"transaction_id\")\n",
    ").orderBy(\"batch_date\")\n",
    "\n",
    "w_vol = Window.orderBy(\"batch_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "volume_with_baseline = (\n",
    "    volume_profiles\n",
    "    .withColumn(\"median_row_count\", F.percentile_approx(\"row_count\", 0.5).over(w_vol))\n",
    "    .withColumn(\"drop_ratio\", F.col(\"row_count\") / F.col(\"median_row_count\"))\n",
    ")\n",
    "\n",
    "volume_with_baseline.orderBy(\"batch_date\").show(40, truncate=False)\n",
    "\n",
    "drop_threshold = 0.5  # less than half the usual volume\n",
    "\n",
    "volume_candidates = volume_with_baseline.where(\n",
    "    F.col(\"drop_ratio\") < drop_threshold\n",
    ")\n",
    "\n",
    "volume_candidates.show(truncate=False)\n",
    "\n",
    "\n",
    "gt_vol = spark.table(\"dq_demo.core.dq_ground_truth\").where(\n",
    "    (F.col(\"table_name\") == \"transactions\") &\n",
    "    (F.col(\"column_name\") == \"*ROW_COUNT*\") &\n",
    "    (F.col(\"anomaly_type\") == \"VOLUME_DROP\")\n",
    ").limit(1)\n",
    "\n",
    "gt_vol_row = gt_vol.collect()[0]\n",
    "gt_vol_id = gt_vol_row[\"id\"]\n",
    "gt_vol_date = gt_vol_row[\"batch_date\"]\n",
    "\n",
    "volume_incidents = (\n",
    "    volume_candidates\n",
    "    .where(F.col(\"batch_date\") == F.lit(gt_vol_date))\n",
    "    .withColumn(\"id\", F.expr(\"uuid()\"))\n",
    "    .withColumn(\"ground_truth_id\", F.lit(gt_vol_id))\n",
    "    .withColumn(\"incident_type\", F.lit(\"VOLUME_DROP\"))\n",
    "    .withColumn(\"severity\", F.lit(\"MEDIUM\"))\n",
    "    .withColumn(\n",
    "        \"details\",\n",
    "        F.concat(\n",
    "            F.lit(\"Volume drop detected for transactions row_count: \"),\n",
    "            F.lit(\"row_count=\"), F.col(\"row_count\").cast(\"string\"),\n",
    "            F.lit(\", median_row_count=\"), F.col(\"median_row_count\").cast(\"string\"),\n",
    "            F.lit(\", drop_ratio=\"), F.col(\"drop_ratio\").cast(\"string\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"detection_ts\", F.current_timestamp())\n",
    "    .select(\n",
    "        \"id\", \"table_name\", \"column_name\", \"batch_date\",\n",
    "        \"ground_truth_id\", \"incident_type\", \"severity\",\n",
    "        \"details\", \"detection_ts\"\n",
    "    )\n",
    ")\n",
    "\n",
    "volume_incidents.write.mode(\"append\").format(\"delta\").saveAsTable(\"dq_demo.core.dq_incidents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56492639-9dec-42a8-81e1-dc6be1504fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.table(\"dq_demo.core.dq_incidents\").orderBy(\"batch_date\", \"incident_type\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "671dd6bd-5494-4022-a62b-415cf437d797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG dq_demo;\n",
    "USE SCHEMA core;\n",
    "\n",
    "WITH gt AS (\n",
    "  SELECT id FROM dq_ground_truth\n",
    "),\n",
    "detected AS (\n",
    "  SELECT DISTINCT ground_truth_id AS id\n",
    "  FROM dq_incidents\n",
    "  WHERE ground_truth_id IS NOT NULL\n",
    "),\n",
    "agg AS (\n",
    "  SELECT\n",
    "    (SELECT COUNT(*) FROM gt) AS total_gt,\n",
    "    (SELECT COUNT(*) FROM detected) AS detected_gt\n",
    ")\n",
    "SELECT\n",
    "  total_gt,\n",
    "  detected_gt,\n",
    "  detected_gt * 1.0 / total_gt AS detection_coverage,\n",
    "  1.0 - detected_gt * 1.0 / total_gt AS silent_failure_rate\n",
    "FROM agg;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b06cc29-c3bf-4e13-991a-201704477edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  i.ground_truth_id,\n",
    "  gt.table_name,\n",
    "  gt.column_name,\n",
    "  gt.batch_date,\n",
    "  i.incident_type,\n",
    "  i.detection_ts,\n",
    "  datediff(SECOND, gt.batch_date, i.detection_ts) AS detection_lag_seconds\n",
    "FROM dq_incidents i\n",
    "JOIN dq_ground_truth gt\n",
    "  ON i.ground_truth_id = gt.id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74f287e-8634-4b57-8249-46f9a1d7fb43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4537815444660364,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_profile_tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
